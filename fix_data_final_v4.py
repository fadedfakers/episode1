import pandas as pd
import numpy as np
import os
import glob
import warnings

warnings.filterwarnings('ignore')

# ================= 1. åŸºç¡€é…ç½® =================
TARGET_COUNTRIES = [
    "United States", "China", "United Kingdom", "Germany", "Japan", 
    "South Korea", "France", "Canada", "India", "United Arab Emirates"
]

# æ˜ å°„è¡¨ï¼šæŠŠå„ç§ç®€å†™ç»Ÿä¸€ä¸ºæ ‡å‡†åç§°
COUNTRY_MAP = {
    "USA": "United States", "US": "United States", "United States of America": "United States",
    "CHN": "China", "People's Republic of China": "China",
    "GBR": "United Kingdom", "UK": "United Kingdom", "Great Britain": "United Kingdom",
    "DEU": "Germany", "Germany (until 1990 former territory of the FRG)": "Germany",
    "JPN": "Japan",
    "KOR": "South Korea", "Korea": "South Korea", "Korea, Rep.": "South Korea",
    "FRA": "France",
    "CAN": "Canada",
    "IND": "India",
    "ARE": "United Arab Emirates", "UAE": "United Arab Emirates"
}

def normalize_country(name):
    clean_name = str(name).strip()
    return COUNTRY_MAP.get(clean_name, clean_name)

# ================= 2. å¢å¼ºç‰ˆè¯»å–å‡½æ•° =================

def load_stanford_flexible(base_path):
    """è¯»å–æ–¯å¦ç¦æ•°æ®ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
    print("ğŸ” Scanning Stanford Data (Deep Search)...")
    data_points = []
    static_scores = {}  # å­˜å‚¨ç´¯è®¡/é™æ€æ•°æ®ï¼ˆç”¨äºåˆ†é…ï¼‰

    # éå†æ‰€æœ‰ CSV
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if not file.endswith(".csv"): continue

            path = os.path.join(root, file)
            try:
                df = pd.read_csv(path)
                cols = [c.strip() for c in df.columns]
                
                # è¯†åˆ«æŒ‡æ ‡ç±»å‹
                is_publication = "1.1." in file
                is_patent = "1.3." in file
                if not (is_publication or is_patent):
                    continue
                
                metric_type = "AI_Publication_Share" if is_publication else "AI_Patent_Share"
                
                # æ¨¡å¼ A: é•¿æ ¼å¼ - æœ‰ Year åˆ—å’Œå›½å®¶åˆ— (Label/Entity/Geographic area)
                country_col = next((c for c in cols if c in ['Label', 'Entity', 'Geographic area', 'Country']), None)
                year_col = next((c for c in cols if 'Year' in c), None)
                
                if country_col and year_col:
                    value_cols = [c for c in cols if c not in [country_col, year_col]]
                    if not value_cols:
                        continue
                    value_col = value_cols[0]
                    
                    for _, row in df.iterrows():
                        c_name = normalize_country(row[country_col])
                        if c_name in TARGET_COUNTRIES:
                            data_points.append({
                                "Country": c_name,
                                "Year": row[year_col],
                                metric_type: row[value_col]
                            })
                            print(f"    Found: {c_name} {row[year_col]} -> {metric_type}")
                
                # æ¨¡å¼ B: é™æ€ç´¯è®¡è¡¨ (Geographic area + æ€»è®¡æ•°) - ç”¨äºå¡«è¡¥
                elif country_col and not year_col:
                    value_cols = [c for c in cols if c != country_col]
                    if value_cols:
                        value_col = value_cols[0]
                        for _, row in df.iterrows():
                            c_name = normalize_country(row[country_col])
                            if c_name in TARGET_COUNTRIES:
                                if c_name not in static_scores:
                                    static_scores[c_name] = {}
                                static_scores[c_name][metric_type] = row[value_col]
                                print(f"    Static score: {c_name} -> {metric_type} = {row[value_col]}")
                
                # æ¨¡å¼ C: å®½æ ¼å¼ - å›½å®¶æ˜¯åˆ—å
                elif 'United States' in cols or 'China' in cols:
                    id_col = df.columns[0]
                    df_melted = df.melt(id_vars=[id_col], var_name='Country', value_name='Value')
                    df_melted.rename(columns={id_col: 'Year'}, inplace=True)
                    for _, row in df_melted.iterrows():
                        c_name = normalize_country(row['Country'])
                        if c_name in TARGET_COUNTRIES:
                            data_points.append({
                                "Country": c_name,
                                "Year": row['Year'],
                                metric_type: row['Value']
                            })

            except Exception as e:
                continue

    print(f"\n  Data points collected: {len(data_points)}")
    print(f"  Static scores collected: {static_scores}")
    
    if not data_points:
        print("  âš ï¸ No time-series Stanford data found!")
        # å¦‚æœæ²¡æœ‰æ—¶åºæ•°æ®ï¼Œç”¨é™æ€åˆ†æ•°ç”Ÿæˆå‡æ—¶é—´åºåˆ—
        if static_scores:
            print("  Using static scores to generate proxy data...")
            years = list(range(2015, 2025))
            for country, scores in static_scores.items():
                for year in years:
                    point = {"Country": country, "Year": year}
                    for metric, value in scores.items():
                        # å½’ä¸€åŒ–ï¼ˆé™¤ä»¥ç¾å›½çš„å€¼ä½œä¸ºä»½é¢ï¼‰
                        us_value = static_scores.get("United States", {}).get(metric, 1)
                        if us_value > 0:
                            point[metric] = value / us_value
                        else:
                            point[metric] = value
                    data_points.append(point)
    
    if not data_points:
        return pd.DataFrame(columns=['Country', 'Year'])
        
    # èšåˆç»“æœ
    df_all = pd.DataFrame(data_points)
    df_all['Year'] = pd.to_numeric(df_all['Year'], errors='coerce')
    
    # ä¸ºç¼ºå¤±çš„å›½å®¶ç”¨é™æ€åˆ†æ•°å¡«è¡¥
    if static_scores:
        for country in TARGET_COUNTRIES:
            if country in static_scores and country not in df_all['Country'].values:
                years = list(range(2015, 2025))
                us_scores = static_scores.get("United States", {})
                for year in years:
                    point = {"Country": country, "Year": year}
                    for metric, value in static_scores[country].items():
                        us_val = us_scores.get(metric, 1)
                        point[metric] = value / us_val if us_val > 0 else value
                    data_points.append(point)
        df_all = pd.DataFrame(data_points)
        df_all['Year'] = pd.to_numeric(df_all['Year'], errors='coerce')
    
    # æŒ‰å›½å®¶å¹´ä»½èšåˆå–å‡å€¼
    agg_cols = [c for c in ['AI_Publication_Share', 'AI_Patent_Share'] if c in df_all.columns]
    if agg_cols:
        df_final = df_all.groupby(['Country', 'Year'])[agg_cols].mean().reset_index()
    else:
        df_final = df_all.groupby(['Country', 'Year']).first().reset_index()
    
    print(f"  âœ“ Final Stanford data: {len(df_final)} rows")
    return df_final

def load_broadband_fixed(filepath):
    """ä¿®å¤å®½å¸¦æ•°æ®è¯»å– (å¤„ç† REF_AREA ä»£ç )"""
    print(f"ğŸŒ Reading Broadband: {filepath}")
    try:
        df = pd.read_csv(filepath)
        print(f"  Columns: {df.columns.tolist()[:10]}...")
        
        # OECD é€šå¸¸ç”¨ REF_AREA å­˜å›½å®¶ä»£ç 
        if 'REF_AREA' in df.columns:
            df['Country'] = df['REF_AREA'].apply(normalize_country)
        elif 'Country' in df.columns:
            df['Country'] = df['Country'].apply(normalize_country)
        
        df = df[df['Country'].isin(TARGET_COUNTRIES)]
        print(f"  Found {len(df)} rows for target countries")
        
        # æå–å…‰çº¤æ•°æ® (å‡è®¾æŒ‡æ ‡ä»£ç åŒ…å« 'FIBRE' æˆ–ç›´æ¥ç”¨æ€»å®½å¸¦)
        # è¿™é‡Œç®€åŒ–ï¼šå¦‚æœæœ‰ FIBRE å°±ç”¨ï¼Œæ²¡æœ‰å°±ç”¨ BroadBand æ€»æ•°
        # å®é™…æ“ä½œï¼šç›´æ¥æŒ‰ Country, Year åˆ†ç»„å–æœ€å¤§å€¼ä½œä¸ºä»£ç†å˜é‡
        df['Year'] = pd.to_numeric(df['TIME_PERIOD'], errors='coerce')
        df['OBS_VALUE'] = pd.to_numeric(df['OBS_VALUE'], errors='coerce')
        df = df.groupby(['Country', 'Year'])['OBS_VALUE'].max().reset_index()
        df.columns = ['Country', 'Year', 'Broadband_Penetration']
        print(f"  âœ“ Processed {len(df)} broadband records")
        return df
    except Exception as e:
        print(f"  âŒ Error: {e}")
        return pd.DataFrame()

def load_existing_data(filepath):
    """è¯»å–å·²æœ‰çš„ v3/v4 æ•°æ®ï¼ˆä¿ç•™ GERD, Electricity ç­‰ï¼‰"""
    print(f"ğŸ“‚ Loading existing data: {filepath}")
    try:
        df = pd.read_csv(filepath)
        print(f"  âœ“ Loaded {len(df)} rows with columns: {df.columns.tolist()}")
        return df
    except Exception as e:
        print(f"  âŒ Error: {e}")
        return pd.DataFrame()

# ================= 3. ä¸»é€»è¾‘ä¸å¡«è¡¥ =================

def main():
    # =============================================
    # ä¿®æ­£åçš„æ–‡ä»¶è·¯å¾„ (æ ¹æ®æ‚¨é¡¹ç›®çš„å®é™…ç»“æ„)
    # =============================================
    base_dir = "."  # å½“å‰ç›®å½• (åæ•°æ¯)
    stanford_path = "The 2025 AI Index Report/1. Research and Development"
    broadband_path = "OECD_å®½å¸¦ä¸ç”µä¿¡.csv"
    existing_data_path = "final_model_data_v4.csv"  # ä½¿ç”¨å·²æœ‰çš„ v4 æ•°æ®ä½œä¸ºåŸºç¡€
    
    print("=" * 60)
    print("ğŸš€ Starting Enhanced Data Merge (fix_data_final_v4)")
    print("=" * 60)
    
    # --- è¯»å–å·²æœ‰æ•°æ® (ä¿ç•™ GERD, Electricity, Supercomputer ç­‰) ---
    df_existing = load_existing_data(existing_data_path)
    
    # --- è¯»å–å„ä¸ªæº ---
    # A. æ–¯å¦ç¦ (AI)
    df_ai = load_stanford_flexible(stanford_path)
    
    # B. å®½å¸¦ (Infrastructure)
    df_bb = load_broadband_fixed(broadband_path)
    
    # --- åˆå¹¶ ---
    print("\nğŸ”— Merging datasets...")
    
    if not df_existing.empty:
        # åŸºäºå·²æœ‰æ•°æ®è¿›è¡Œå¢å¼º
        final = df_existing.copy()
        
        # åˆå¹¶ AI æ•°æ®
        if not df_ai.empty:
            # åªæ›´æ–°ç©ºå€¼
            for col in ['AI_Publication_Share', 'AI_Patent_Share']:
                if col in df_ai.columns:
                    if col not in final.columns:
                        final[col] = np.nan
                    # Merge and fill
                    merged = pd.merge(final[['Country', 'Year']], df_ai[['Country', 'Year', col]], 
                                     on=['Country', 'Year'], how='left', suffixes=('', '_new'))
                    if f'{col}_new' in merged.columns:
                        final[col] = final[col].fillna(merged[f'{col}_new'])
                    elif col in merged.columns:
                        final[col] = final[col].fillna(merged[col])
        
        # åˆå¹¶å®½å¸¦æ•°æ®
        if not df_bb.empty and 'Broadband_Penetration' in df_bb.columns:
            if 'Broadband_Penetration' not in final.columns:
                final['Broadband_Penetration'] = np.nan
            merged = pd.merge(final[['Country', 'Year']], df_bb[['Country', 'Year', 'Broadband_Penetration']], 
                             on=['Country', 'Year'], how='left', suffixes=('', '_new'))
            if 'Broadband_Penetration_new' in merged.columns:
                final['Broadband_Penetration'] = final['Broadband_Penetration'].fillna(merged['Broadband_Penetration_new'])
    else:
        # ä»å¤´æ„å»º
        years = range(2010, 2026)
        skeleton = pd.DataFrame([(c, y) for c in TARGET_COUNTRIES for y in years], columns=['Country', 'Year'])
        final = pd.merge(skeleton, df_ai, on=['Country', 'Year'], how='left')
        final = pd.merge(final, df_bb, on=['Country', 'Year'], how='left')
    
    # ================= 4. å…³é”®ï¼šå¼ºåŠ›å¡«è¡¥ (Imputation) =================
    print("\nğŸ”§ Running Smart Imputation...")
    
    # è§„åˆ™ 1: çº¿æ€§æ’å€¼ (å¡«è¡¥ä¸­é—´ç©ºç¼º)
    final = final.sort_values(['Country', 'Year'])
    numeric_cols = final.select_dtypes(include=[np.number]).columns.tolist()
    if 'Year' in numeric_cols:
        numeric_cols.remove('Year')
    
    for col in numeric_cols:
        final[col] = final.groupby('Country')[col].transform(lambda x: x.interpolate(limit_direction='both'))
    
    # è§„åˆ™ 2: å¯¹äºå®Œå…¨ç¼ºå¤±çš„å›½å®¶ (å¦‚å°åº¦çš„ GERDï¼Œé˜¿è”é…‹çš„ AI)ï¼Œä½¿ç”¨ Tortoise åˆ†æ•°æ˜ å°„
    # æ˜ å°„é€»è¾‘ï¼šCountry_Value = US_Value * (Country_Score / US_Score) * Correction_Factor
    
    # è®¾å®šåŸºå‡†å€¼ (åŸºäºç¾å›½ 2023 å¹´æ•°æ®çš„ä¼°ç®—)
    us_ai_share = 0.15  # å‡è®¾ç¾å›½ AI è®ºæ–‡å æ¯”çº¦ 15%
    us_broadband = 40.0  # å‡è®¾ç¾å›½å®½å¸¦æ¸—é€ç‡
    
    tortoise_scores = {
        'India': {'AI': 0.14, 'Infra': 0.15},  # 0-1 å½’ä¸€åŒ–åçš„åˆ†æ•°
        'United Arab Emirates': {'AI': 0.13, 'Infra': 0.29},
        'China': {'AI': 0.48, 'Infra': 0.66}
    }
    
    print("  Applying Tortoise-based imputation for missing countries...")
    for idx, row in final.iterrows():
        ctry = row['Country']
        
        # å¡«è¡¥ AI æ•°æ®
        if 'AI_Publication_Share' in final.columns:
            if pd.isna(row.get('AI_Publication_Share')) and ctry in tortoise_scores:
                final.at[idx, 'AI_Publication_Share'] = us_ai_share * (tortoise_scores[ctry]['AI'] / 1.0) 
                
        if 'AI_Patent_Share' in final.columns:
            if pd.isna(row.get('AI_Patent_Share')) and ctry in tortoise_scores:
                ai_pub = final.at[idx, 'AI_Publication_Share'] if 'AI_Publication_Share' in final.columns else us_ai_share * 0.1
                final.at[idx, 'AI_Patent_Share'] = ai_pub * 0.8  # ä¸“åˆ©é€šå¸¸æ¯”è®ºæ–‡å°‘
            
        # å¡«è¡¥å®½å¸¦æ•°æ®
        if 'Broadband_Penetration' in final.columns:
            if pd.isna(row.get('Broadband_Penetration')) and ctry in tortoise_scores:
                final.at[idx, 'Broadband_Penetration'] = us_broadband * (tortoise_scores[ctry]['Infra'] / 1.0)

    # è§„åˆ™ 3: æœ€åçš„å…œåº• (ç”¨åˆ—å‡å€¼å¡«å……ï¼Œé˜²æ­¢ä»£ç æŠ¥é”™)
    print("  Final fillna with column means...")
    for col in numeric_cols:
        if final[col].isna().any():
            col_mean = final[col].mean()
            if pd.notna(col_mean):
                final[col] = final[col].fillna(col_mean)
            else:
                final[col] = final[col].fillna(0)
    
    # ä¿å­˜
    output_path = "final_model_data_v4_ready.csv"
    final.to_csv(output_path, index=False)
    
    print("\n" + "=" * 60)
    print(f"âœ… Done! File saved as '{output_path}'")
    print("=" * 60)
    
    # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
    print("\nğŸ“Š Data Quality Report:")
    print(f"  Total rows: {len(final)}")
    print(f"  Countries: {final['Country'].nunique()}")
    print(f"  Year range: {final['Year'].min()} - {final['Year'].max()}")
    print(f"\n  Missing values per column:")
    missing = final.isnull().sum()
    for col, count in missing.items():
        status = "âœ“" if count == 0 else "âš ï¸"
        print(f"    {status} {col}: {count}")
    
    print("\nğŸ“‹ Sample data (first 15 rows):")
    print(final.head(15).to_string())

if __name__ == "__main__":
    main()
